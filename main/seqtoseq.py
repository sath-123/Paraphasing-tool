# -*- coding: utf-8 -*-
"""seqtoseq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14DgbilQzTWYugkgEaVUqtk6nvP-sP7cc
"""

from google.colab import drive
drive.mount('/content/drive')

import json
from operator import itemgetter
from itertools import groupby
import numpy as np
import sys
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

import torch
import torch.nn.functional as F
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import pandas as pd
from sklearn.model_selection import train_test_split
import re
import time
import json

file = open('/content/drive/MyDrive/Project_files/final_hindi_data_train.json', 'r',encoding='utf-8')
x1 = file.read()
x1 = json.loads(x1)
sortkeyfn = itemgetter('image_id')
x1['annotations'].sort(key=sortkeyfn)
captions = []
s=0
for key,valuesiter in groupby(x1['annotations'], key=sortkeyfn):
    captions.append(dict(type=key, items=list(v['caption'] for v in valuesiter)))
    s=s+1

# reading the validation data
file2 = open('/content/drive/MyDrive/Project_files/final_hindi_data_validation.json', 'r',encoding='utf-8')
x1 = file2.read()
x1 = json.loads(x1)
sortkeyfn = itemgetter('image_id')
x1['annotations'].sort(key=sortkeyfn)
captions_val = []
s=0
for key,valuesiter in groupby(x1['annotations'], key=sortkeyfn):
    captions_val.append(dict(type=key, items=list(v['caption'] for v in valuesiter)))
    s=s+1

inputs=[]
outputs=[]
paraphases=[]
inputs_test=[]
for x in range(len(captions)):
  sentence = []
  for y in range(len(captions[x]['items'][0])):
    sentence.append(captions[x]['items'][0][y])
  paraphases.append(sentence)

for x in paraphases:
  inputs.append(x[0])
  inputs.append(x[2])
  inputs.append(x[0])
  outputs.append(x[1])
  outputs.append(x[3])
  outputs.append(x[2])
  inputs_test.append(x[3])

'''
Tokenization for inputs
'''
def Tokenization(w):
    w = re.sub(r"([?.!,])", r" \1 ", w)
    w = w.strip()
    return w.split()

input_tokens=[]
output_tokens=[]
input_vocab=[]
output_vocab=[]
for i in range(len(inputs)):
  words=Tokenization(inputs[i])
  tokens=[]
  tokens.append("<s>")
  tokens=tokens+words
  tokens.append("<e>")
  input_tokens.append(tokens)
  input_vocab+=words
  words=Tokenization(outputs[i])
  tokens=[]
  tokens.append("<s>")
  tokens=tokens+words
  tokens.append("<e>")
  output_tokens.append(tokens)
  input_vocab+=words

input_vocab.append("<s>")
input_vocab.append("<e>")
input_vocab.append("<unk>")
input_vocab.append("<pad>")

inputV_to_txt={}
key=0
for i in range(len(input_vocab)):
  if input_vocab[i] in inputV_to_txt:
    continue
  else :
    inputV_to_txt[input_vocab[i]]=key
    key+=1

number_to_txt={}
for x in inputV_to_txt:
  number_to_txt[inputV_to_txt[x]]=x
print(len(inputV_to_txt))

input_maxlength=0
output_maxlength=0
max_length=0
for i in range(len(input_tokens)):
  if len(input_tokens[i])>input_maxlength:
    input_maxlength=len(input_tokens[i])
  if len(output_tokens[i])>output_maxlength:
    output_maxlength=len(output_tokens[i])

if input_maxlength>output_maxlength:
  max_length=input_maxlength
else:
  max_length=output_maxlength

for i in range(len(input_tokens)):
  input_tokens[i]=input_tokens[i]+["<pad>"]*(max_length-len(input_tokens[i]))
  output_tokens[i]=output_tokens[i]+["<pad>"]*(max_length-len(output_tokens[i]))

final_input=[]
final_output=[]
for i in range(len(input_tokens)):
  word_T=[]
  for word in input_tokens[i]:
    if inputV_to_txt.get(word)!=None:
      word_T.append(int(inputV_to_txt[word]))
    else :
      word_T.append(int(inputV_to_txt.get("<unk>")))
  final_input.append(word_T)
  word_S=[]
  for word in output_tokens[i]:
    if word in inputV_to_txt:
      word_S.append(int(inputV_to_txt[word]))
      # print(outputV_to_txt[word],word)
    else :
      word_S.append(int(inputV_to_txt.get("<unk>")))
    # print(word_S)
  
  final_output.append(word_S)

final_input=np.array(final_input)
final_output=np.array(final_output)
f_input = torch.LongTensor(final_input).to(device)
f_output = torch.LongTensor(final_output).to(device)

batch_wiseinput=torch.split(f_input,128)
batch_wiseoutput=torch.split(f_output,128)

batchs_f=[]
for i in range(0,len(batch_wiseinput)):
  batchs_f.append((batch_wiseinput[i],batch_wiseoutput[i]))
print(len(batchs_f))
print(len(batchs_f[0]))

class Encoder(nn.Module):
    
    def __init__(self, input_size, embedding_size, hidden_size, decoder_hidden_size, num_layers, dropout):
        super(Encoder, self).__init__()
        self.hidden_size=hidden_size
        self.n_layers=1
        self.dropout = nn.Dropout(dropout).to(device)
        self.embedding = nn.Embedding(input_size, embedding_size).to(device)
        self.rnn = nn.LSTM(embedding_size, hidden_size).to(device)

    def forward(self,x):
        embedding = self.dropout(self.embedding(x)).to(device)
        # print(np.shape(embedding),"embedding")
        outputs,(hidden,cell) = self.rnn(embedding)
        return hidden,cell

class Decoder(nn.Module):
    
    def __init__(
        self, embedding_size, encoding_hidden_size, hidden_size, output_size, num_layers, dropout):
        super(Decoder, self).__init__()
        self.output_size = output_size
        self.dropout = nn.Dropout(dropout).to(device)
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.embedding = nn.Embedding(output_size, embedding_size).to(device)
        self.rnn = nn.LSTM(embedding_size, hidden_size).to(device)
        self.fc = nn.Linear(hidden_size, output_size).to(device)

    def forward(self, x, hidden, cell):
        x = x.unsqueeze(0)
        embedding = self.dropout(self.embedding(x)).to(device)
        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))
        predictions = self.fc(outputs.squeeze(0))
        return predictions, hidden, cell

'''
Creating the Model
'''
class Seq2Seq(nn.Module):
    
    def __init__(self, encoder, decoder):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        
    def forward(self, source, target, teacher_force_ratio=0.5):
        batch_size = source.shape[1]
        target_len = target.shape[0]
        target_vocab_size = self.decoder.output_size
        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)
        hidden, cell = self.encoder(source)
        x = target[0]
        for t in range(1, target_len):
            output, hidden,cell = self.decoder(x, hidden, cell)
            outputs[t] = output
            teacher_force = random.random() < teacher_force_ratio
            best_guess = output.argmax(1)
            x = target[t] if random.random() < teacher_force else best_guess
        return outputs

'''
Important Parmeters
'''
num_epochs = 10 
learning_rate = 0.03
batch_size = 128

'''
HyperParameters
'''
load_model = False
device = torch.device("cuda" if torch.cuda.is_available() else 'cpu')
input_size_encoder = len(inputV_to_txt)
input_size_decoder = len(inputV_to_txt)
output_size = len(inputV_to_txt)
num_layers = 1
enc_dropout = 0.5
dec_dropout = 0.5
print(output_size)

encoder_net = Encoder(input_size_encoder, 128,256,256,1, enc_dropout).to(device)
decoder_net = Decoder(128, 256, 256, output_size, num_layers, dec_dropout).to(device)

from sklearn import model_selection
'''
Initiating Model
'''
def init_weights(m):
    for name, param in m.named_parameters():
        nn.init.uniform_( param.data, -0.08, 0.08 )

model = Seq2Seq(encoder_net, decoder_net).to(device)
model.apply( init_weights )
'''
Using Adam Optimizer
'''
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

pad_idx = inputV_to_txt["<pad>"]
criterion = nn.CrossEntropyLoss().to(device)

# training the model
def train(model, iterator, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0
    initial=100
    length=len(batchs_f)
    dummy=0
    for i, batch in enumerate(iterator):
        if i>0 and i==length*2/3:
          break
        # print(i)
        source = batch[0]
        target = batch[1]
        optimizer.zero_grad()
        output = model( source, target ).to(device)
        output_dim = output.shape[-1]
        output = output[1:].view( -1, output_dim )
        target = target[1:].view( -1 )
        loss = criterion( output, target )
        loss.backward()
        optimizer.step()
        dummy=i
        epoch_loss += loss.item()
    print(epoch_loss /dummy,"training loss")
    model.eval()
    # validation 
    val_loss=0
    for i,batch in enumerate(iterator):
      if i>0 and i>length*2/3:
        source = batch[0]
        target = batch[1]
        output = model( source, target ).to(device)
        output_dim = output.shape[-1]
        output = output[1:].view( -1, output_dim )
        target = target[1:].view( -1 )
        loss = criterion( output, target )
        val_loss += loss.item()
    if initial > val_loss:
      initial=val_loss
      torch.save(model.state_dict(),'/content/drive/MyDrive/Project_files/seqtoseq.pth')

    print(val_loss /(len(iterator)-dummy),"validation loss")

for x in range(5):
  train(model,batchs_f,optimizer,criterion,1)

# finding paraphase for a input sentence
sentence="एक बेसबॉल मैदान के ऊपर बेसबॉल की एक गेम खेलने वाले पुरुषों का एक समूह"
words_index=[]
sentence=Tokenization(sentence)
print(sentence)
sentence=[inputV_to_txt[token] for token in sentence]
sentence.insert(0, inputV_to_txt["<s>"])
sentence.append(inputV_to_txt["<e>"])
sentence_tensor = torch.LongTensor(sentence).unsqueeze(1).to(device)
with torch.no_grad():
        hidden, cell = model.encoder(sentence_tensor)
print(inputV_to_txt["<s>"])
outputs = [inputV_to_txt["<s>"]]
for _ in range(len(sentence_tensor)):
    previous_word = torch.LongTensor([outputs[-1]]).to(device)
    with torch.no_grad():
        output, hidden = model.decoder(previous_word, hidden, cell)
        best_guess = output.argmax(1).item()
    outputs.append(best_guess)
    if output.argmax(1).item() == inputV_to_txt["<e>"]:
      print("yuhh")
      break
print(outputs)
paraphased_sentence = [number_to_txt[idx] for idx in outputs]
print(paraphased_sentence[1:])

model.load_state_dict(torch.load('/content/drive/MyDrive/Project_files/seqtoseq.pth'))

# calculating performance
from nltk.translate.bleu_score import sentence_bleu
actual=[]
predicted=[]
for x in range(len(inputs_test)):
  sentence=Tokenization(inputs_test[x])
  actual=actual+sentence
  # print(sentence)
  sentence=[inputV_to_txt[token] for token in sentence]
  sentence.insert(0, inputV_to_txt["<s>"])
  sentence.append(inputV_to_txt["<e>"])
  sentence_tensor = torch.LongTensor(sentence).unsqueeze(1).to(device)
  with torch.no_grad():
          hidden, cell = model.encoder(sentence_tensor)
  # print(inputV_to_txt["<s>"])
  outputs = [inputV_to_txt["<s>"]]
  for _ in range(len(sentence_tensor)):
      previous_word = torch.LongTensor([outputs[-1]]).to(device)
      with torch.no_grad():
          output, hidden,cell = model.decoder(previous_word, hidden, cell)
          best_guess = output.argmax(1).item()
      outputs.append(best_guess)
      if output.argmax(1).item() == inputV_to_txt["<e>"]:
        # print("yuhh")
        break

  paraphased_sentence = [number_to_txt[idx] for idx in outputs]
  predicted+=paraphased_sentence
  # print(paraphased_sentence)

import nltk
from nltk.translate import meteor_score
nltk.download('wordnet')

score = meteor_score.meteor_score([predicted],actual)
print("METEOR score:", score*100)

from nltk.translate.bleu_score import sentence_bleu
bleu_score = sentence_bleu(actual,predicted)
print("BLEU score: {:.2f}".format(bleu_score*100))

